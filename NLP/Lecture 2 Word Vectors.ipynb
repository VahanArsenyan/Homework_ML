{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2: word vector representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import string\n",
    "import operator\n",
    "import itertools\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you define a couple of tokenizers and use them on a toy sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence_1 = 'The quick brown fox jumps over the lazy dog.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - delimiter tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_by_split(text):\n",
    "    \"\"\"Tokenizes a given string of text by splitting words by whitespace\"\"\"\n",
    "    tokens = text.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "assert tokenize_by_split(test_sentence_1) == ['The', 'quick', 'brown', 'fox', \n",
    "                                              'jumps', 'over', 'the', 'lazy', 'dog.']\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punkt_and_tokenize_by_split(text):\n",
    "    \"\"\"Replaces punktuation from given string of text with whitespace, then\n",
    "    tokenizes it by splitting words by whitespace\"\"\"\n",
    "    punkt_symbols = string.punctuation\n",
    "    for sym in punkt_symbols:\n",
    "        text = text.replace(sym, \" \")\n",
    "    tokens = text.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "assert remove_punkt_and_tokenize_by_split(test_sentence_1) == ['The', 'quick', 'brown', 'fox', \n",
    "                                                               'jumps', 'over', 'the', 'lazy', 'dog']\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - using regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence_2 = \"This is a test that isn't so simple: 1.23\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_by_regex(text):\n",
    "    \"\"\"Tokenizes a given string of text by applying the 'tokenize' method \n",
    "    of the provided 'tokenizer' object\"\"\"\n",
    "    tokenizer = nltk.RegexpTokenizer('\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "assert tokenize_by_regex(test_sentence_2) == ['This', 'is', 'a', 'test', 'that', \n",
    "                                              'isn', 't', 'so', 'simple', '1', '23']\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - using an advanced tokenizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_by_punkt_model(text):\n",
    "    \"\"\"Tokenizes a given string of text by applying the NLTK Punkt tokenizer model.\n",
    "    Uses nltk.word_tokenize method\"\"\"\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "assert tokenize_by_punkt_model(test_sentence_2) == ['This', 'is', 'a', 'test', 'that', \n",
    "                                                    'is', \"n't\", 'so', 'simple', ':', '1.23']\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: n-grams and stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University. First of all, let's get it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\KiriToK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Brown corpus contains texts from different categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.brown.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentences from each category can be accessed as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4637\n"
     ]
    }
   ],
   "source": [
    "adv_sents = list(nltk.corpus.brown.sents(categories='adventure'))\n",
    "print(len(adv_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Dan',\n",
       "  'Morgan',\n",
       "  'told',\n",
       "  'himself',\n",
       "  'he',\n",
       "  'would',\n",
       "  'forget',\n",
       "  'Ann',\n",
       "  'Turner',\n",
       "  '.'],\n",
       " ['He', 'was', 'well', 'rid', 'of', 'her', '.']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_sents[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what are the most frequent unigrams in the 'adventure' category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joins a list of lists of tokens into a one large string of text\n",
    "adventure_text = ' '.join(list(itertools.chain.from_iterable(adv_sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses the tokenizer function we've just written to tokenize text\n",
    "adventure_tokens = tokenize_by_regex(adventure_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60004\n"
     ]
    }
   ],
   "source": [
    "# turns each token to lowercase (simple normalization technique)\n",
    "lowered_tokens = [token.lower() for token in adventure_tokens]\n",
    "print(len(lowered_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts the number of occurances for each unigram\n",
    "word_counter = collections.Counter(lowered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABI4AAAJCCAYAAACmkYxsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncWRmZmZmZmZmZlX/Ab2IirGTbTkVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2bbb49ed6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = [w[0] for w in word_counter.most_common(35)]\n",
    "values = [w[1] for w in word_counter.most_common(35)]\n",
    "indexes = np.arange(len(labels))\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(indexes, values)\n",
    "plt.xticks(indexes, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that some of the most common words above are not very interesting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stop-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exersise you remove stopwords, find the most frequent bigrams, then display them on a barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\KiriToK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the list of english stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "print(len(stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - filtering stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords from the list of 'lowered_tokens'\n",
    "# your code goes here\n",
    "stopword_filtered_tokens = [t for t in lowered_tokens if t not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - getting the bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn your filtered list of unigrams into a list of bigrams, joint by whitespace\n",
    "# to achieve that, use the function nltk.ngrams(your_tokens, 2)\n",
    "# your code goes here\n",
    "filtered_bigrams = list(nltk.ngrams(stopword_filtered_tokens, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitlered_merged_bigrams = [' '.join(t) for t in filtered_bigrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - counting occurances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# now count the occurances of bigrams using a new Counter instance\n",
    "# your code goes here\n",
    "bigram_counter = collections.Counter(fitlered_merged_bigrams)\n",
    "\n",
    "assert {'miss langford', 'mary jane', 'billy tilghman'}.issubset(set(map(operator.itemgetter(0), \n",
    "                                                                         bigram_counter.most_common(15))))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIEAAAJCCAYAAABXmtfhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAEIgAAAAgAEIgQAAAAAG8P8DplHURsMSUdoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2bbb4a12ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = [w[0] for w in bigram_counter.most_common(15)]\n",
    "values = [w[1] for w in bigram_counter.most_common(15)]\n",
    "indexes = np.arange(len(labels))\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(indexes, values)\n",
    "plt.xticks(indexes, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home Exercise 1: vocabularies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you write a function that builds a vocabulary from the provided text corpus. Then you use it to encode tokens into numeric form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - building a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(tokens, max_size):\n",
    "    \"\"\"\n",
    "    Builds a dictionary of at most max_size most frequent tokens from the supplied list of tokens.\n",
    "    More frequent tokens should have a lower id, but that is not strictly required.\n",
    "    Two special symbols 'NULL':0 and 'UNKN':1 should also be added to the dictionary.\n",
    "    \n",
    "    EXAMPLE:\n",
    "    {\n",
    "        'NULL': 0,\n",
    "        'UNKN': 1,\n",
    "        'the': 2,\n",
    "        'and': 3,\n",
    "        'a': 4,\n",
    "        ...\n",
    "    }\n",
    "    \"\"\"\n",
    "    vocabulary = {}\n",
    "    reserved_symbols = [\"NULL\", \"UNKN\"]\n",
    "    \n",
    "    # your code goes here\n",
    "\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-8c002368e6da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmy_vocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlowered_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVOC_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_vocabulary\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mVOC_SIZE\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32massert\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'NULL'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'UNKN'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missubset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_vocabulary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVOC_SIZE\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missubset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_vocabulary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "VOC_SIZE = 5000\n",
    "\n",
    "my_vocabulary = build_vocabulary(lowered_tokens, VOC_SIZE)\n",
    "\n",
    "assert len(my_vocabulary) == VOC_SIZE\n",
    "assert {'NULL', 'UNKN'}.issubset(set(my_vocabulary.keys()))\n",
    "assert set([w[0] for w in word_counter.most_common(VOC_SIZE-10)]).issubset(set(my_vocabulary.keys()))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - encoding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_tokens(sentence, tokenizer, token_to_id, max_len):\n",
    "    \"\"\"\n",
    "    Converts a list of tokens to a list of token ids using the supplied dictionary.\n",
    "    Pads resulting list with NULL identifiers up to max_len length.\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    \n",
    "    # STEP 1: convert sentence to a list of tokens\n",
    "    # your code goes here\n",
    "    \n",
    "    # STEP 2: replace tokens with their identifiers from the vocabulary\n",
    "    # If the token is not present in the vocabulary, replace it with UNKN identifier\n",
    "\n",
    "    # STEP 3: pad the sequence id's with NULL identifiers until so that it's length is equal to max_len\n",
    "\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-36b7937261d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m                               tokenize_by_regex, my_vocabulary, MAX_LEN)\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorized\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mMAX_LEN\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m assert [my_vocabulary.get(t, my_vocabulary['UNKN']) \n\u001b[0;32m      8\u001b[0m         for t in tokenize_by_regex(test_sentence)] + [0]*(MAX_LEN-len(tokenize_by_regex(test_sentence))) == vectorized\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MAX_LEN = 16\n",
    "test_sentence = 'The animals thundered away into the moonlight , heading for the ridges .'\n",
    "vectorized = vectorize_tokens(test_sentence,\n",
    "                              tokenize_by_regex, my_vocabulary, MAX_LEN)\n",
    "\n",
    "assert len(vectorized) == MAX_LEN\n",
    "assert [my_vocabulary.get(t, my_vocabulary['UNKN']) \n",
    "        for t in tokenize_by_regex(test_sentence)] + [0]*(MAX_LEN-len(tokenize_by_regex(test_sentence))) == vectorized\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you create a function to compute sentence similarity, then build a simple Information Retrieval system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4637\n"
     ]
    }
   ],
   "source": [
    "VOC_SIZE = 5000\n",
    "\n",
    "adv_brown_sents = [' '.join(sent) for sent in nltk.corpus.brown.sents(categories='adventure')]\n",
    "print(len(adv_brown_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the CountVectorizer instance\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=VOC_SIZE, stop_words=stopwords, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words={'you', 'both', 'who', 'himself', 'other', 'our', 'same', \"mustn't\", 'was', 'about', 'most', 're', 'only', 'the', \"you're\", 'yourself', \"mightn't\", 'into', 'she', 'against', 'no', 'themselves', \"hadn't\", 'couldn', 't', 'weren', 'for', 'did', 'me', \"shouldn't\", \"shan't\", \"hasn't\", 'will', ... 'not', 'during', 'o', 'a', 'yourselves', 'from', 'shouldn', 'have', 'which', 'now', 'being', 'too'},\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# builds the vocabulary from the data\n",
    "tfidf_vectorizer.fit(adv_brown_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_vocab = dict(zip(range(len(tfidf_vectorizer.get_feature_names())),\n",
    "                                  tfidf_vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4637, 5000)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applies one-hot encoding to the provided data, transforming sentences into vectors\n",
    "vectorized_sents = tfidf_vectorizer.transform(adv_brown_sents)\n",
    "\n",
    "# the resulting matrix has shape (N_SAMPLES x VOC_SIZE)\n",
    "vectorized_sents.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the sentence vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dan Morgan told himself he would forget Ann Turner .\n"
     ]
    }
   ],
   "source": [
    "# sentence\n",
    "print(adv_brown_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# sentence vector is almost all zeroes\n",
    "print(vectorized_sents[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4932 4389 4251 2021 1219  751  102]\n"
     ]
    }
   ],
   "source": [
    "# nonzero elements of the sentence vector\n",
    "print(vectorized_sents[0].nonzero()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['would', 'turner', 'told', 'morgan', 'forget', 'dan', 'ann']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the words are the same, but the word order is lost, and stopwords are removed\n",
    "[tfidf_vectorizer_vocab[wid] for wid in vectorized_sents[0].nonzero()[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.20980297]]\n"
     ]
    }
   ],
   "source": [
    "# we can now compute the similarity between sentences like so:\n",
    "sent1vector = vectorized_sents[0]\n",
    "sent10vector = vectorized_sents[10]\n",
    "similarity = cosine_similarity(sent1vector, sent10vector)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_sentence_similarity(sent1, sent2, vectorizer):\n",
    "    \"\"\"Encodes provided sentences using the 'vectorizer' object,\n",
    "    then computes the cosine similarity between sentence vectors\n",
    "    Outputs a real number between [0,1] \"\"\"\n",
    "    \n",
    "    # CountVectorizer requires a list of sentences as input\n",
    "    sent1 = [sent1]\n",
    "    sent2 = [sent2]\n",
    "    vec_sent1 = tfidf_vectorizer.transform(sent1)\n",
    "    vec_sent2 = tfidf_vectorizer.transform(sent2)\n",
    "    # your code goes here\n",
    "    similarity = cosine_similarity(vec_sent1, vec_sent2)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence_1 = 'I like building robots'\n",
    "test_sentence_2 = 'I also like building pillow fortresses'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "assert onehot_sentence_similarity(test_sentence_1, test_sentence_2, tfidf_vectorizer) > 0.5\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - information retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchEngine(object):\n",
    "    def __init__(self, knowledge_base, voc_size=5000):\n",
    "        \"\"\"\n",
    "        Implements a simple information retrieval system based on Tf-Idf text representation.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.kbase = np.array(knowledge_base)\n",
    "        self.vectorizer = TfidfVectorizer(max_features=voc_size)\n",
    "        self.vectorized_kbase = self.vectorizer.fit_transform(knowledge_base)\n",
    "        \n",
    "    def search(self, query, top_k=3):\n",
    "        \"\"\"\n",
    "        Retrieves the top-k documents from the knowledge_base most similar to given query\n",
    "        \"\"\"\n",
    "        \n",
    "        vectorized_query = self.vectorizer.transform([query])\n",
    "        \n",
    "        # your code goes here\n",
    "        # STEP 1: compute similarities between query and all documents in knowledge base\n",
    "        result = cosine_similarity(vectorized_query, self.vectorized_kbase)[0]\n",
    "        # STEP 2: sort the similarities to find most similar document indices\n",
    "        # HINT: use np.argsort to do that\n",
    "        res_ind = np.argsort(-result)\n",
    "        # STEP 3: gets top-k most similar documents from self.kbase, returns them\n",
    "        results = self.kbase[res_ind[:top_k]]\n",
    "        return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = SearchEngine(adv_brown_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "query1 = 'take it easy'\n",
    "result1 = se.search(query1, top_k=3)[0]\n",
    "assert query1 in result1\n",
    "\n",
    "query2 = 'uneasy feeling'\n",
    "result2 = se.search(query2, top_k=1)[0]\n",
    "assert query2 in result2\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Home Exercise 1: language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you build a 3-gram language model, then use it to generate grammaticaly valid text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 57340/57340 [00:05<00:00, 10671.28it/s]\n"
     ]
    }
   ],
   "source": [
    "model = collections.defaultdict(lambda: collections.defaultdict(lambda: 0))\n",
    " \n",
    "for sentence in tqdm(nltk.corpus.brown.sents()):\n",
    "    for w1, w2, w3 in nltk.trigrams(sentence, pad_right=True, pad_left=True):\n",
    "        # iterate over all trigrams, accumulate co-occurance counts\n",
    "        model[(w1, w2)][w3] += 1\n",
    "\n",
    "for w1_w2 in model:\n",
    "    total_count = float(sum(model[w1_w2].values()))\n",
    "    for w3 in model[w1_w2]:\n",
    "        # normalize counts to produce a valid probability distribution\n",
    "        model[w1_w2][w3] /= total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Man had had a 3-foot anaconda that weighed only 9.8 ounces .\n"
     ]
    }
   ],
   "source": [
    "text = [None, None]\n",
    " \n",
    "sentence_finished = False\n",
    " \n",
    "while not sentence_finished:\n",
    "    # introduce a stochastic variable\n",
    "    r = random.random()\n",
    "    accumulator = .0\n",
    " \n",
    "    for word in model[tuple(text[-2:])].keys():\n",
    "        pr = model[tuple(text[-2:])][word]\n",
    "        accumulator += pr\n",
    "        \n",
    "        # frequent trigrams are more likely to overflow accumulator:\n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    " \n",
    "    if text[-2:] == [None, None]:\n",
    "        sentence_finished = True\n",
    "\n",
    "print(' '.join([t for t in text if t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 By analogy, implement a 4-gram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
